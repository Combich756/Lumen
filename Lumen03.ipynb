{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HyYH383aBQUP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# –í–°–¢–ê–í–¨ –°–Æ–î–ê –°–í–û–ò –ó–ù–ê–ß–ï–ù–ò–Ø\n",
        "os.environ[\"TELEGRAM_BOT_TOKEN\"] = \"8250825015:AAE9nIh5RLmbjNFl2yS0m3sBUKhfi3VJXd8\"\n",
        "os.environ[\"GIGACHAT_CREDENTIALS\"] = \"MmI2ZGMxMzktNDRmNC00ODYyLWJhYzQtZWY1YWFhY2RiYjAwOjI2OGEwNGVjLWYwNTQtNDUwMC05MjJhLTYwOTk3YWMzYzJhNA==\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-text-splitters\n",
        "!pip install -qU langchain-community faiss-cpu\n",
        "!pip install -qU sentence-transformers\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hzC2-R8E4sp6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-OnWxGuixEQ",
        "outputId": "00b41af5-a706-4f1a-fe75-a61b6813ce7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-telegram-bot==20.8 in /usr/local/lib/python3.12/dist-packages (20.8)\n",
            "Requirement already satisfied: langchain-gigachat==0.3.13a2 in /usr/local/lib/python3.12/dist-packages (0.3.13a2)\n",
            "Requirement already satisfied: langgraph>=1.0.4 in /usr/local/lib/python3.12/dist-packages (1.0.4)\n",
            "Requirement already satisfied: langchain>=1.0.0 in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: python-dotenv==1.0.1 in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: openpyxl==3.1.5 in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: nest_asyncio==1.6.0 in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pandas==2.2.3 in /usr/local/lib/python3.12/dist-packages (2.2.3)\n",
            "Requirement already satisfied: httpx~=0.26.0 in /usr/local/lib/python3.12/dist-packages (from python-telegram-bot==20.8) (0.26.0)\n",
            "Requirement already satisfied: gigachat<0.2.0,>=0.1.43 in /usr/local/lib/python3.12/dist-packages (from langchain-gigachat==0.3.13a2) (0.1.43)\n",
            "Requirement already satisfied: langchain-core<2.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-gigachat==0.3.13a2) (1.1.0)\n",
            "Requirement already satisfied: types-requests<3.0,>=2.32 in /usr/local/lib/python3.12/dist-packages (from langchain-gigachat==0.3.13a2) (2.32.4.20250913)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl==3.1.5) (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.3) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas==2.2.3) (2025.2)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.4) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.4) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.4) (0.2.10)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.4) (2.12.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=1.0.4) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx~=0.26.0->python-telegram-bot==20.8) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx~=0.26.0->python-telegram-bot==20.8) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx~=0.26.0->python-telegram-bot==20.8) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx~=0.26.0->python-telegram-bot==20.8) (3.11)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from httpx~=0.26.0->python-telegram-bot==20.8) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx~=0.26.0->python-telegram-bot==20.8) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0,>=1.0->langchain-gigachat==0.3.13a2) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0,>=1.0->langchain-gigachat==0.3.13a2) (0.4.47)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0,>=1.0->langchain-gigachat==0.3.13a2) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0,>=1.0->langchain-gigachat==0.3.13a2) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0,>=1.0->langchain-gigachat==0.3.13a2) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0,>=1.0->langchain-gigachat==0.3.13a2) (4.15.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph>=1.0.4) (1.12.0)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph>=1.0.4) (3.11.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph>=1.0.4) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph>=1.0.4) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph>=1.0.4) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.3) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=2 in /usr/local/lib/python3.12/dist-packages (from types-requests<3.0,>=2.32->langchain-gigachat==0.3.13a2) (2.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0,>=1.0->langchain-gigachat==0.3.13a2) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=1.0->langchain-gigachat==0.3.13a2) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=1.0->langchain-gigachat==0.3.13a2) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=1.0->langchain-gigachat==0.3.13a2) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0,>=1.0->langchain-gigachat==0.3.13a2) (3.4.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install \\\n",
        "  python-telegram-bot==20.8 \\\n",
        "  langchain-gigachat==0.3.13a2 \\\n",
        "  \"langgraph>=1.0.4\" \\\n",
        "  \"langchain>=1.0.0\" \\\n",
        "  python-dotenv==1.0.1 \\\n",
        "  openpyxl==3.1.5 \\\n",
        "  nest_asyncio==1.6.0 \\\n",
        "  pandas==2.2.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import asyncio\n",
        "import logging\n",
        "import re\n",
        "from io import BytesIO\n",
        "from typing import Sequence\n",
        "\n",
        "import pandas as pd\n",
        "from typing_extensions import Annotated, TypedDict\n",
        "\n",
        "from telegram import Update, InputFile\n",
        "from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes\n",
        "\n",
        "from langchain_gigachat.chat_models import GigaChat\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_core.messages import HumanMessage, BaseMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, END, StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 0. –ë–∞–∑–æ–≤–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "# --------------------------------------------------------------------\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∏: —á–∏—Ç–∞–µ–º —Ç–æ–∫–µ–Ω—ã –∏–∑ –æ–∫—Ä—É–∂–µ–Ω–∏—è\n",
        "# --------------------------------------------------------------------\n",
        "TELEGRAM_BOT_TOKEN = os.environ.get(\"TELEGRAM_BOT_TOKEN\")\n",
        "GIGACHAT_CREDENTIALS = os.environ.get(\"GIGACHAT_CREDENTIALS\")\n",
        "\n",
        "if not TELEGRAM_BOT_TOKEN:\n",
        "    raise RuntimeError(\"–ù–µ –∑–∞–¥–∞–Ω TELEGRAM_BOT_TOKEN –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è\")\n",
        "\n",
        "if not GIGACHAT_CREDENTIALS:\n",
        "    raise RuntimeError(\"–ù–µ –∑–∞–¥–∞–Ω GIGACHAT_CREDENTIALS –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è\")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 2. IT-—Ä–µ—Å—É—Ä—Å—ã –¥–ª—è –º–∏–Ω–∏-RAG\n",
        "# --------------------------------------------------------------------\n",
        "IT_RESOURCES = [\n",
        "    {\n",
        "        \"topic\": \"python_beginner\",\n",
        "        \"title\": \"MIT OCW: Introduction to Computer Science and Programming in Python\",\n",
        "        \"url\": \"https://ocw.mit.edu/courses/6-0001-introduction-to-computer-science-and-programming-in-python-fall-2016/\",\n",
        "        \"lang\": \"EN\",\n",
        "        \"level\": \"beginner\",\n",
        "        \"description\": \"–í–≤–æ–¥–Ω—ã–π –∫—É—Ä—Å –ø–æ –æ—Å–Ω–æ–≤–∞–º –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∏ –Ω–∞ Python.\"\n",
        "    },\n",
        "    {\n",
        "        \"topic\": \"cs_core\",\n",
        "        \"title\": \"MIT OCW: Introductory Programming Collection\",\n",
        "        \"url\": \"https://ocw.mit.edu/collections/introductory-programming/\",\n",
        "        \"lang\": \"EN\",\n",
        "        \"level\": \"intermediate\",\n",
        "        \"description\": \"–ü–æ–¥–±–æ—Ä–∫–∞ –±–∞–∑–æ–≤—ã—Ö –∫—É—Ä—Å–æ–≤ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–º –Ω–∞—É–∫–∞–º.\"\n",
        "    },\n",
        "    {\n",
        "        \"topic\": \"cs_many_courses\",\n",
        "        \"title\": \"500+ Free Computer Science Courses\",\n",
        "        \"url\": \"https://dev.to/manocormen/500-free-computer-science-online-courses-from-the-world-s-top-cs-universities-3pg5\",\n",
        "        \"lang\": \"EN\",\n",
        "        \"level\": \"mixed\",\n",
        "        \"description\": \"–ë–æ–ª—å—à–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –æ–Ω–ª–∞–π–Ω-–∫—É—Ä—Å–æ–≤ –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ –æ—Ç –≤–µ–¥—É—â–∏—Ö –≤—É–∑–æ–≤.\"\n",
        "    },\n",
        "    {\n",
        "        \"topic\": \"cs_1200_courses\",\n",
        "        \"title\": \"1200 Free Computer Science Courses\",\n",
        "        \"url\": \"https://www.freecodecamp.org/news/free-courses-top-cs-universities/\",\n",
        "        \"lang\": \"EN\",\n",
        "        \"level\": \"mixed\",\n",
        "        \"description\": \"–°–ø–∏—Å–æ–∫ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö CS-–∫—É—Ä—Å–æ–≤ –æ—Ç —Ç–æ–ø–æ–≤—ã—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤.\"\n",
        "    },\n",
        "    {\n",
        "        \"topic\": \"python_ru\",\n",
        "        \"title\": \"–ö—É—Ä—Å: –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≤—Å–µ—Ö (Python, RU-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å)\",\n",
        "        \"url\": \"https://www.coursera.org/learn/python-ru\",\n",
        "        \"lang\": \"RU\",\n",
        "        \"level\": \"beginner\",\n",
        "        \"description\": \"–ù–∞—á–∞–ª—å–Ω—ã–π –∫—É—Ä—Å –ø–æ Python —Å —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–º –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–º.\"\n",
        "    },\n",
        "    {\n",
        "        \"topic\": \"it_ru_platform\",\n",
        "        \"title\": \"Stepik ‚Äì IT-–∫—É—Ä—Å—ã\",\n",
        "        \"url\": \"https://stepik.org/catalog/search?tag=programming\",\n",
        "        \"lang\": \"RU\",\n",
        "        \"level\": \"mixed\",\n",
        "        \"description\": \"–ö–∞—Ç–∞–ª–æ–≥ –∫—É—Ä—Å–æ–≤ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –∏ –¥—Ä—É–≥–∏–º IT-–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º.\"\n",
        "    },\n",
        "    {\n",
        "    \"topic\": \"cs_ossu_path\",\n",
        "    \"title\": \"OSSU: Computer Science Curriculum\",\n",
        "    \"url\": \"https://github.com/ossu/computer-science\",\n",
        "    \"lang\": \"EN\",\n",
        "    \"level\": \"mixed\",\n",
        "    \"description\": \"–ü–æ–ª–Ω–∞—è –¥–æ—Ä–æ–∂–Ω–∞—è –∫–∞—Ä—Ç–∞ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–º –Ω–∞—É–∫–∞–º –∏–∑ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –æ–Ω–ª–∞–π–Ω‚Äë–∫—É—Ä—Å–æ–≤ –≤–µ–¥—É—â–∏—Ö —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤.\"\n",
        "  },\n",
        "  {\n",
        "      \"topic\": \"web_fullstack_en\",\n",
        "      \"title\": \"freeCodeCamp: Full Stack Web Development\",\n",
        "      \"url\": \"https://www.freecodecamp.org\",\n",
        "     \"lang\": \"EN\",\n",
        "      \"level\": \"beginner\",\n",
        "      \"description\": \"–ë–µ—Å–ø–ª–∞—Ç–Ω–∞—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ –ø–æ –≤–µ–±‚Äë—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∏ —Å–º–µ–∂–Ω—ã–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º —Å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–º–∏ –∑–∞–¥–∞–Ω–∏—è–º–∏ –∏ –ø—Ä–æ–µ–∫—Ç–∞–º–∏.\"\n",
        "  },\n",
        "  {\n",
        "      \"topic\": \"cs_university_free_en\",\n",
        "      \"title\": \"Free University CS Courses (Open Culture / freeCodeCamp list)\",\n",
        "      \"url\": \"https://www.freecodecamp.org/news/free-courses-top-cs-universities/\",\n",
        "      \"lang\": \"EN\",\n",
        "      \"level\": \"mixed\",\n",
        "      \"description\": \"–ü–æ–¥–±–æ—Ä–∫–∞ —Å–æ—Ç–µ–Ω –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –∫—É—Ä—Å–æ–≤ –ø–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ –æ—Ç —Ç–æ–ø‚Äë—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–æ–≤ –º–∏—Ä–∞.\"\n",
        "  },\n",
        "  {\n",
        "     \"topic\": \"cs_harvard_free\",\n",
        "     \"title\": \"Harvard Online: Free Courses (CS & Programming)\",\n",
        "      \"url\": \"https://pll.harvard.edu/catalog/free\",\n",
        "     \"lang\": \"EN\",\n",
        "      \"level\": \"mixed\",\n",
        "      \"description\": \"–ö–∞—Ç–∞–ª–æ–≥ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –æ–Ω–ª–∞–π–Ω‚Äë–∫—É—Ä—Å–æ–≤ –ì–∞—Ä–≤–∞—Ä–¥–∞, –≤–∫–ª—é—á–∞—è –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –∏ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–º –Ω–∞—É–∫–∞–º.\"\n",
        "  },\n",
        "  {\n",
        "      \"topic\": \"cs_openlearn\",\n",
        "      \"title\": \"OpenLearn: Digital & Computing\",\n",
        "      \"url\": \"https://www.open.edu/openlearn/digital/free-courses\",\n",
        "      \"lang\": \"EN\",\n",
        "      \"level\": \"beginner\",\n",
        "      \"description\": \"–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –∏–Ω—Ç—Ä–æ‚Äë–∫—É—Ä—Å—ã –ø–æ –∫–æ–º–ø—å—é—Ç–µ—Ä–∞–º, –ò–¢ –∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –æ—Ç Open University.\"\n",
        "  },\n",
        "  {\n",
        "    \"topic\": \"ru_yandex_practicum_free\",\n",
        "    \"title\": \"–Ø–Ω–¥–µ–∫—Å –ü—Ä–∞–∫—Ç–∏–∫—É–º: –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –∫—É—Ä—Å—ã –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é\",\n",
        "    \"url\": \"https://practicum.yandex.ru/catalog/programming/free/\",\n",
        "    \"lang\": \"RU\",\n",
        "    \"level\": \"beginner\",\n",
        "    \"description\": \"–ü–æ–¥–±–æ—Ä–∫–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –≤–≤–æ–¥–Ω—ã—Ö –∫—É—Ä—Å–æ–≤ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –æ—Ç –Ø–Ω–¥–µ–∫—Å –ü—Ä–∞–∫—Ç–∏–∫—É–º–∞.\"\n",
        "  },\n",
        "  {\n",
        "      \"topic\": \"ru_hexlet_free\",\n",
        "      \"title\": \"Hexlet: –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –∫—É—Ä—Å—ã –ø–æ IT‚Äë–ø—Ä–æ—Ñ–µ—Å—Å–∏—è–º\",\n",
        "      \"url\": \"https://ru.hexlet.io/courses_free\",\n",
        "      \"lang\": \"RU\",\n",
        "      \"level\": \"beginner\",\n",
        "      \"description\": \"–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ –∫—É—Ä—Å—ã –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –∏ —Å–º–µ–∂–Ω—ã–º IT‚Äë–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º —Å —É–ø–æ—Ä–æ–º –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫—É.\"\n",
        "  },\n",
        "  {\n",
        "      \"topic\": \"ru_netology_free\",\n",
        "      \"title\": \"–ù–µ—Ç–æ–ª–æ–≥–∏—è: –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –ø–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ\",\n",
        "      \"url\": \"https://netology.ru/free/development\",\n",
        "      \"lang\": \"RU\",\n",
        "      \"level\": \"beginner\",\n",
        "      \"description\": \"–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –≤–≤–æ–¥–Ω—ã–µ –∫—É—Ä—Å—ã, –≤–µ–±–∏–Ω–∞—Ä—ã –∏ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –ø–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∏ —Å–º–µ–∂–Ω—ã–º digital‚Äë–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º.\"\n",
        "  },\n",
        "  {\n",
        "      \"topic\": \"ru_code_basics\",\n",
        "      \"title\": \"CodeBasics: –æ—Å–Ω–æ–≤—ã –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è\",\n",
        "      \"url\": \"https://code-basics.com/ru\",\n",
        "      \"lang\": \"RU\",\n",
        "      \"level\": \"beginner\",\n",
        "      \"description\": \"–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –ø–æ –æ—Å–Ω–æ–≤–∞–º —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è: Python, JavaScript, Go –∏ –¥—Ä.\"\n",
        "  }\n",
        "\n",
        "]\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 3. –ü–∞—Ä—Å–∏–Ω–≥ Markdown-—Ç–∞–±–ª–∏—Ü—ã –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ Excel\n",
        "# --------------------------------------------------------------------\n",
        "def parse_markdown_table_to_df(text: str) -> pd.DataFrame:\n",
        "    lines = text.strip().split('\\n')\n",
        "    table_lines = []\n",
        "    table_started = False\n",
        "\n",
        "    for line in lines:\n",
        "        stripped = line.strip()\n",
        "        if not table_started:\n",
        "            if stripped.startswith('|') and len(stripped) > 3:\n",
        "                table_started = True\n",
        "            else:\n",
        "                continue\n",
        "        if stripped.startswith('|') and stripped.endswith('|'):\n",
        "            cells = [cell.strip() for cell in stripped.split('|')[1:-1]]\n",
        "            if any(cell for cell in cells):\n",
        "                table_lines.append(cells)\n",
        "        elif table_started:\n",
        "            break\n",
        "\n",
        "    if len(table_lines) < 2:\n",
        "        raise ValueError(\"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ç—Ä–æ–∫ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã (—Ç—Ä–µ–±—É–µ—Ç—Å—è –∑–∞–≥–æ–ª–æ–≤–æ–∫ + ‚â•1 —Å—Ç—Ä–æ–∫–∞)\")\n",
        "\n",
        "    headers = table_lines[0]\n",
        "    data_rows = [\n",
        "        row for row in table_lines[1:]\n",
        "        if not all(re.fullmatch(r'-+', cell) for cell in row)\n",
        "    ]\n",
        "\n",
        "    if not data_rows:\n",
        "        raise ValueError(\"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–∞–±–ª–∏—Ü–µ (—Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å)\")\n",
        "\n",
        "    return pd.DataFrame(data_rows, columns=headers)\n",
        "\n",
        "\n",
        "def extract_links(text: str) -> list:\n",
        "    LINK_PATTERN = r'\\[([^\\]]+)\\]\\(\\s*([^)]*?)\\s*\\)'\n",
        "    matches = re.findall(LINK_PATTERN, text)\n",
        "    links = [{\"title\": title.strip(), \"url\": url.strip()} for title, url in matches]\n",
        "\n",
        "    text_without_links = re.sub(LINK_PATTERN, '', text).strip()\n",
        "    if text_without_links and not re.match(r'^[\\s,]*$', text_without_links):\n",
        "        return [{\"context\": text_without_links, \"links\": links}] if links else [{\"raw\": text.strip()}]\n",
        "\n",
        "    return links if links else [{\"raw\": text.strip()}]\n",
        "\n",
        "\n",
        "def df_to_excel_bytes(df: pd.DataFrame) -> BytesIO:\n",
        "    try:\n",
        "        df_out = df.copy()\n",
        "\n",
        "        # –°–ª—É–∂–µ–±–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞, –∫–∞–∫ –∏ —Ä–∞–Ω—å—à–µ\n",
        "        if '–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã' in df_out.columns:\n",
        "            df_out['resources'] = df_out['–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã'].apply(extract_links)\n",
        "\n",
        "        if '–ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å' in df_out.columns:\n",
        "            def parse_duration(s):\n",
        "                if not s:\n",
        "                    return 1\n",
        "                m = re.search(r'(\\d+)', s)\n",
        "                return int(m.group(1)) if m else 1\n",
        "\n",
        "            df_out['duration_weeks'] = df_out['–ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å'].apply(parse_duration)\n",
        "\n",
        "        if '–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –¥–∞—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è' in df_out.columns:\n",
        "            def parse_week_range(s):\n",
        "                if not s:\n",
        "                    return {\"start\": 1, \"end\": 1}\n",
        "                m = re.search(r'(\\d+)(?:\\s*-\\s*(\\d+))?', s)\n",
        "                if m:\n",
        "                    start = int(m.group(1))\n",
        "                    end = int(m.group(2)) if m.group(2) else start\n",
        "                    return {\"start\": start, \"end\": end}\n",
        "                return {\"raw\": s}\n",
        "\n",
        "            df_out['completion'] = df_out['–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –¥–∞—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è'].apply(parse_week_range)\n",
        "\n",
        "        # üëâ –û—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Ç–æ–ª—å–∫–æ –≤–∏–¥–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã\n",
        "        hidden_cols = ['resources', 'duration_weeks', 'completion']\n",
        "        visible_cols = [c for c in df_out.columns if c not in hidden_cols]\n",
        "\n",
        "        from openpyxl.utils import get_column_letter\n",
        "\n",
        "        output = BytesIO()\n",
        "        with pd.ExcelWriter(output, engine='openpyxl') as writer:\n",
        "            # –ü–∏—à–µ–º —Ç–æ–ª—å–∫–æ –≤–∏–¥–∏–º—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –∫–æ–ª–æ–Ω–∫–∏\n",
        "            df_out[visible_cols].to_excel(writer, sheet_name='–ü–ª–∞–Ω', index=False)\n",
        "\n",
        "            worksheet = writer.sheets['–ü–ª–∞–Ω']\n",
        "            for i, col in enumerate(visible_cols, start=1):\n",
        "                max_len = max(df_out[col].astype(str).map(len).max(), len(col)) + 2\n",
        "                worksheet.column_dimensions[get_column_letter(i)].width = min(max_len, 50)\n",
        "\n",
        "        output.seek(0)\n",
        "        return output\n",
        "\n",
        "    except Exception:\n",
        "        logging.exception(\"–û—à–∏–±–∫–∞ –≤ df_to_excel_bytes\")\n",
        "        raise\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 4. –ú–æ–¥–µ–ª—å GigaChat (—á–∞—Ç) –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
        "# --------------------------------------------------------------------\n",
        "model = GigaChat(\n",
        "    credentials=GIGACHAT_CREDENTIALS,\n",
        "    scope=\"GIGACHAT_API_PERS\",\n",
        "    model=\"GigaChat-Max\",\n",
        "    verify_ssl_certs=False,\n",
        ")\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={\"device\": \"cpu\"},\n",
        ")\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 5. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –ø–æ IT_RESOURCES\n",
        "# --------------------------------------------------------------------\n",
        "docs = []\n",
        "for res in IT_RESOURCES:\n",
        "    content = (\n",
        "        f\"{res['title']}. {res['description']} \"\n",
        "        f\"–Ø–∑—ã–∫: {res['lang']}. –£—Ä–æ–≤–µ–Ω—å: {res['level']}.\"\n",
        "    )\n",
        "    docs.append(Document(page_content=content, metadata=res))\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50,\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs)\n",
        "\n",
        "vectorstore = FAISS.from_documents(doc_splits, embeddings)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 6. –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è RAG-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø–æ –æ–±—É—á–µ–Ω–∏—é IT\n",
        "# --------------------------------------------------------------------\n",
        "RAG_SYSTEM_PROMPT = (\n",
        "    \"–¢—ã ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –æ–±—É—á–µ–Ω–∏—é –≤ —Å—Ñ–µ—Ä–µ IT. \"\n",
        "    \"–¢–≤–æ—è –æ—Å–Ω–æ–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ ‚Äî —Å–æ—Å—Ç–∞–≤–ª—è—Ç—å –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —É—á–µ–±–Ω—ã–µ –ø–ª–∞–Ω—ã –≤ –≤–∏–¥–µ \"\n",
        "    \"—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π Markdown-—Ç–∞–±–ª–∏—Ü—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, \"\n",
        "    \"–ò–°–ü–û–õ–¨–ó–£–Ø –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–µ—Å—É—Ä—Å–æ–≤ (–∫—É—Ä—Å—ã, –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã, –ø–æ–¥–±–æ—Ä–∫–∏).\\n\\n\"\n",
        "    \"–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö IT-—Ä–µ—Å—É—Ä—Å–æ–≤:\\n{context}\\n\\n\"\n",
        "    \"–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ç–∞–±–ª–∏—Ü–µ:\\n\"\n",
        "    \"1. –¢–∞–±–ª–∏—Ü–∞ –î–û–õ–ñ–ù–ê –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å–æ —Å—Ç—Ä–æ–∫–∏ '### –£—á–µ–±–Ω—ã–π –ø–ª–∞–Ω' –∏ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –†–û–í–ù–û 7 –∫–æ–ª–æ–Ω–æ–∫ –≤ —ç—Ç–æ–º –ø–æ—Ä—è–¥–∫–µ:\\n\"\n",
        "    \"   ‚Ññ | –¢–µ–º–∞/–ú–æ–¥—É–ª—å | –¶–µ–ª—å –∏–∑—É—á–µ–Ω–∏—è | –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã | –§–æ—Ä–º–∞—Ç –∑–∞–Ω—è—Ç–∏–π | –ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å | –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –¥–∞—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è\\n\"\n",
        "    \"2. –í –∫–æ–ª–æ–Ω–∫–µ '–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ —Ä–µ—Å—É—Ä—Å—ã' –∏—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —Ñ–æ—Ä–º–∞—Ç —Å—Å—ã–ª–æ–∫ Markdown: [–ù–∞–∑–≤–∞–Ω–∏–µ](URL).\\n\"\n",
        "    \"   –ï—Å–ª–∏ —Ä–µ—Å—É—Ä—Å–æ–≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ ‚Äî –ø–µ—Ä–µ—á–∏—Å–ª–∏ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é: [A](url1), [B](url2).\\n\"\n",
        "    \"3. –í –∫–æ–ª–æ–Ω–∫–µ '–ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å' —É–∫–∞–∑—ã–≤–∞–π –¢–û–õ–¨–ö–û —á–∏—Å–ª–æ –∏ –µ–¥–∏–Ω–∏—Ü—É: '1 –Ω–µ–¥–µ–ª—è', '2 –Ω–µ–¥–µ–ª–∏', '3 –Ω–µ–¥–µ–ª–∏'.\\n\"\n",
        "    \"4. –í –∫–æ–ª–æ–Ω–∫–µ '–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –¥–∞—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è' –∏—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º–∞—Ç: '–Ω–µ–¥–µ–ª—è 1', '–Ω–µ–¥–µ–ª–∏ 2-3', '–Ω–µ–¥–µ–ª–∏ 8-9'.\\n\"\n",
        "    \"5. –ü–µ—Ä–µ–¥ —Ç–µ–º –∫–∞–∫ —Å—Ç—Ä–æ–∏—Ç—å –ø–ª–∞–Ω, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∑–∞–¥–∞–π —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.\\n\"\n",
        "    \"6. –ü–æ–¥–±–∏—Ä–∞—è —Ä–µ—Å—É—Ä—Å—ã, –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å –∏—Å–ø–æ–ª—å–∑—É–π —Ç–µ, —á—Ç–æ –µ—Å—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.\"\n",
        ")\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", RAG_SYSTEM_PROMPT),\n",
        "    MessagesPlaceholder(variable_name=\"messages\"),\n",
        "])\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 7. –°–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è LangGraph –∏ —É–∑–ª—ã RAG-–≥—Ä–∞—Ñ–∞\n",
        "# --------------------------------------------------------------------\n",
        "class RAGState(TypedDict):\n",
        "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
        "    context: str\n",
        "\n",
        "def retrieve_node(state: RAGState) -> dict:\n",
        "    last_human = None\n",
        "    for m in reversed(state[\"messages\"]):\n",
        "        if isinstance(m, HumanMessage):\n",
        "            last_human = m\n",
        "            break\n",
        "    query = last_human.content if last_human else \"\"\n",
        "    docs = retriever.invoke(query)\n",
        "    context_text = \"\\n\\n\".join(d.page_content for d in docs)\n",
        "    return {\"context\": context_text}\n",
        "\n",
        "def call_rag_model(state: RAGState) -> dict:\n",
        "    chain = rag_prompt | model\n",
        "    response = chain.invoke(state)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 8. –°–±–æ—Ä–∫–∞ RAG-–≥—Ä–∞—Ñ–∞ —Å –ø–∞–º—è—Ç—å—é\n",
        "# --------------------------------------------------------------------\n",
        "workflow = StateGraph(state_schema=RAGState)\n",
        "workflow.add_node(\"retrieve\", retrieve_node)\n",
        "workflow.add_node(\"model\", call_rag_model)\n",
        "\n",
        "workflow.add_edge(START, \"retrieve\")\n",
        "workflow.add_edge(\"retrieve\", \"model\")\n",
        "workflow.add_edge(\"model\", END)\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 9. Telegram-–æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏\n",
        "# --------------------------------------------------------------------\n",
        "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    await update.message.reply_text(\n",
        "        \"–ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç Lumen. –Ø –ø–æ–º–æ–≥—É —Ç–µ–±–µ —Å–æ—Å—Ç–∞–≤–∏—Ç—å IT-—É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω üìö\\n\"\n",
        "        \"–ù–∞–ø–∏—à–∏, —á—Ç–æ —Ö–æ—á–µ—à—å –∏–∑—É—á–∏—Ç—å –∏ –∑–∞ –∫–∞–∫–æ–µ –≤—Ä–µ–º—è.\"\n",
        "    )\n",
        "\n",
        "async def clear(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    user_id = str(update.effective_user.id)\n",
        "    config = {\"configurable\": {\"thread_id\": user_id}}\n",
        "    await app.aupdate_state(config, {\"messages\": []})\n",
        "    await update.message.reply_text(\"–ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞!\")\n",
        "\n",
        "async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
        "    user_id = str(update.effective_user.id)\n",
        "    text = (update.message.text or \"\").strip()\n",
        "    config = {\"configurable\": {\"thread_id\": user_id}}\n",
        "\n",
        "    thinking_msg = await update.message.reply_text(\"–ü–æ–∂–∞–ª—É–π—Å—Ç–∞ –ø–æ–¥–æ–∂–¥–∏—Ç–µ... ‚è≥\")\n",
        "\n",
        "    try:\n",
        "        # –û–¥–∏–Ω –≤—ã–∑–æ–≤ RAG-–≥—Ä–∞—Ñ–∞\n",
        "        output = await asyncio.wait_for(\n",
        "            app.ainvoke(\n",
        "                {\"messages\": [HumanMessage(content=text)], \"context\": \"\"},\n",
        "                config,\n",
        "            ),\n",
        "            timeout=90.0,\n",
        "        )\n",
        "\n",
        "        plan_text = output[\"messages\"][-1].content\n",
        "\n",
        "        # –ü—ã—Ç–∞–µ–º—Å—è —Å–¥–µ–ª–∞—Ç—å Excel –∏–∑ –æ—Ç–≤–µ—Ç–∞\n",
        "        try:\n",
        "            df = parse_markdown_table_to_df(plan_text)\n",
        "            excel_bytes = df_to_excel_bytes(df)\n",
        "            await thinking_msg.delete()\n",
        "            await update.message.reply_document(\n",
        "                document=InputFile(excel_bytes, filename=\"plan.xlsx\"),\n",
        "                caption=\"–í–∞—à —É—á–µ–±–Ω—ã–π –ø–ª–∞–Ω –≤ Excel\",\n",
        "            )\n",
        "        except Exception:\n",
        "            logging.exception(\"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ Excel\")\n",
        "            await thinking_msg.edit_text(plan_text[:4000])\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        err_text = \"\".join(traceback.format_exception_only(type(e), e))\n",
        "        logging.exception(f\"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞ –æ—Ç {user_id}\")\n",
        "        try:\n",
        "            await thinking_msg.edit_text(\"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞:\\n\" + err_text[:3500])\n",
        "        except Exception:\n",
        "            await update.message.reply_text(\"–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞:\\n\" + err_text[:3500])\n",
        "\n",
        "# --------------------------------------------------------------------\n",
        "# 10. –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –ø–æ–¥ Colab / Jupyter)\n",
        "# --------------------------------------------------------------------\n",
        "async def main():\n",
        "    application = Application.builder().token(TELEGRAM_BOT_TOKEN).build()\n",
        "\n",
        "    application.add_handler(CommandHandler(\"start\", start))\n",
        "    application.add_handler(CommandHandler(\"clear\", clear))\n",
        "    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))\n",
        "\n",
        "    print(\"‚úÖ –ë–æ—Ç –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è... –ù–∞–ø–∏—à–∏—Ç–µ –µ–º—É –≤ Telegram.\")\n",
        "    await application.initialize()\n",
        "    await application.start()\n",
        "    await application.updater.start_polling()\n",
        "    print(\"‚úÖ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω –∏ —Å–ª—É—à–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è\")\n",
        "\n",
        "# –ó–∞–ø—É—Å–∫ —Å —É—á—ë—Ç–æ–º Colab / Jupyter\n",
        "loop = asyncio.get_event_loop()\n",
        "if loop.is_running():\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "    asyncio.create_task(main())\n",
        "else:\n",
        "    asyncio.run(main())\n"
      ],
      "metadata": {
        "id": "oj4SEu_nJePo"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}
